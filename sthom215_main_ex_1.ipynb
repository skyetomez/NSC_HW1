{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title HW 1 \n",
    "# @markdown Student Info\n",
    "# @markdown Name: [Skyler Thomas]\n",
    "# @markdown JHED-ID: [sthom215]\n",
    "\n",
    "# @markdown Load up the calcium image video in the file: TEST MOVIE 00001-small-motion.tif. This file represents the raw fluorescence video in TIF format, common for calcium imaging.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "home = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A \n",
    "Write a script to play the data as a video (for Python users, look up the package plotly). Observe the changes in the video as a function of time and how the neurons “wiggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "\n",
    "tiff1 = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "\n",
    "tiffreader = TIFF_IO(home / tiff1)\n",
    "movie = tiffreader.read_tiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title View a frame\n",
    "# plt.figure(figsize=(4,4), tight_layout=True)\n",
    "data = movie[301:400, :,:]\n",
    "px.imshow(data, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\")) # @param i {type:\"integer\"}\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "Pick two frames that seem to be particularly offset from each other. Compute their correlation, and repeat for different shifts in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title load tiff file using the tifffile module\n",
    "import tifffile\n",
    "\n",
    "dir = os.listdir()\n",
    "tiff1 = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "\n",
    "if tiff1 not in dir:\n",
    "  cwd = home /'drive/MyDrive/NSC_HW1'\n",
    "  movie = tifffile.imread(cwd/tiff1)\n",
    "else:\n",
    "  movie = tifffile.imread(home/tiff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose two frames to get peak correlation:\n",
    "from sthom215_problem_1_ex_2 import XCORR\n",
    "frame1 = 32 # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "frame2 = 52 # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "xcor = XCORR(movie[frame1,:,:], movie[frame2,:,:])\n",
    "tmp = xcor.get_xcorr_fft()\n",
    "\n",
    "print(f'max_xcorr is: {tmp[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcor.plot_xcorr_surface_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcor.plot_xcorr_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Summary images. \n",
    "\n",
    "Load up the calcium image video in the file: $\\texttt{TEST MOVIE 00001-small.tif}$. This file represents the raw fluorescence video in TIF format, but without the motion. One of the most common ways that people analyze functional data is to try to identify individual cells in the data in a “summary image”. A summary image condenses an entire video sequence into a single image, wherein each pixel represents a summary of the entire time-trace at the location of that pixel in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A \n",
    "\n",
    "Plot the “mean”, “median”, and “variance” images for the data provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title define functions\n",
    "# Plot the “mean”, “median”, and “variance” images for the data provided. \n",
    "from sthom215_problem_2_ex_1 import SummaryImages\n",
    "\n",
    "fname = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "\n",
    "summaryimages1 = SummaryImages(home/fname, fnorm=False)\n",
    "summaryimages2 = SummaryImages(home/fname, fnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryimages1.plot_mean_median_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryimages2.plot_mean_median_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice about the visible cells in each of these?\n",
    "\n",
    "The cells in the mean and median image cells look similar. This suggests that the median and mean pixels are near each other. In the variance image, no cells are visible. This suggests that after mean centering, the majority of the pixels were set to zero while only the intense pixels remained. Additionally, the variance may not all be due to cells in the image. Some of the bright spots might be artifacts from how the video was recorded or large changes in pixels due to shifts across frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Summary images. \n",
    "\n",
    "Load up the calcium image video in the file: $\\texttt{TEST MOVIE 00001-small.tif}$. This file represents the raw fluorescence video in TIF format, but without the motion. One of the most common ways that people analyze functional data is to try to identify individual cells in the data in a “summary image”. A summary image condenses an entire video sequence into a single image, wherein each pixel represents a summary of the entire time-trace at the location of that pixel in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "\n",
    "* ### What would you expect a good statistic for a summary image to capture?\n",
    "\n",
    "\n",
    "A good statistic for a summary image should provide an overview of the content across all of the frames in movies across time. So I should have an idea of not only color but of shape and even changes in hue.\n",
    "\n",
    "\n",
    "* ### What other statistics would you think could work? \n",
    "\n",
    "\n",
    "An image that summarizes the average shape across all images could be a summary. An image that summarizes the average edges across all images could also work. A plot that shows the average distribution across pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "\n",
    "tiff1 = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "\n",
    "tiffreader = TIFF_IO(home / tiff1)\n",
    "movie = tiffreader.read_tiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Get pixel distribution at a single pixel across the entire movie\n",
    "_, height, width = movie.shape\n",
    "\n",
    "x_coord = 0 # @param {type:\"slider\", min:0, max:width, step:1}\n",
    "y_coord = 0 # @param {type:\"slider\", min:0, max:height, step:1}\n",
    "\n",
    "from sthom215_problem_2_ex_2 import get_pixelDistribution\n",
    "#sigma controls smoothing of the distribution\n",
    "get_pixelDistribution(movie, 2 ,1, sigma=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Edge detection summary\n",
    "from sthom215_problem_2_ex_2 import get_edge_mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(get_edge_mean(movie), cmap='gray')\n",
    "plt.title(\"Average Edge Intensity of Cellular Activity Over Time\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Were you right? \n",
    "\n",
    "I think my pixel approach is interesting but the scale is too small to be useful across an entire movie. It is better suited to studying the statistics of an image at a single frame. \n",
    "\n",
    "I think my edge detetction approach is a decent summary because it gives the viewer an idea of active areas of change across all of the frames in the movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Drawing Regions Of Interest (ROIs) \n",
    "\n",
    "Given the summary images, the next step is to draw ROIs around the image to isolate the pixels belonging to a specific cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: \n",
    "\n",
    "* Write a short script that takes in a summary image and potentially other information (i.e., a seed pixel selected by the user), and outputs a binary mask over an ROI. \n",
    "\n",
    "* Find 5 different ROIs in the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load the video file using the provided TIFF_IO class.\n",
    "from sthom215_problem_2_ex_1 import SummaryImages\n",
    "from sthom215_problem_3_ex_1 import get_roi_2, get_bounding_boxes\n",
    "fname = 'TEST_MOVIE_00001-small.tif'\n",
    "summaryimage = SummaryImages(home/fname)\n",
    "summaryimage = summaryimage.get_mean()\n",
    "sigma = 3; kernel = 7\n",
    "ROIs = get_roi_2(summaryimage, sigma, kernel)\n",
    "rois_bounding_boxes = get_bounding_boxes(ROIs, True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Drawing Regions Of Interest (ROIs) \n",
    "\n",
    "Given the summary images, the next step is to draw ROIs around the image to isolate the pixels belonging to a specific cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: \n",
    "\n",
    "* How might you assess how good your script did?\n",
    "\n",
    "I can assess how well my script did by comparing my ROI to the raw image to see how well it captures the details or separates the ROI from the rest of the image frame. This visual inspection would cary in its accuracy betwen me an someone who is an expert at determining what is a neuron in an image and someone who is novice like myself. So, I could also have other people come and visually inspect the output of the script to determine whether or not the script did a good job.\n",
    "\n",
    "This could also be done staistically by looking at the how much overlap there is between the ROI in the mask and the original image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Time-trace estimation\n",
    "\n",
    "With the ROIs, time-traces need to be extracted by identifying the relative brightness of that region\n",
    "of interest in each frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A:\n",
    "\n",
    "* Write a function that identifies, for the ROIs identified in Problem 2, their time-traces.\n",
    "\n",
    "* Describe your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load the video file using the provided TIFF_IO class.\n",
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "\n",
    "fname = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "tiffreader = TIFF_IO(home/fname)\n",
    "movie = tiffreader.read_tiff()\n",
    "print(\"movie has shape:\", movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sthom215_problem_3_ex_1 import get_hw3_roi #get rois from exercise 3\n",
    "from sthom215_problem_4_ex_1 import create_mask_for_bounding_box, ROI\n",
    "import plotly.express as px\n",
    "box,mask = create_mask_for_bounding_box(movie, ROI[3])\n",
    "px.imshow(box[0:100], animation_frame=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sthom215_problem_3_ex_1 import get_hw3_roi #get rois from exercise 3\n",
    "from sthom215_problem_4_ex_1 import create_mask_for_bounding_box, ROI\n",
    "import plotly.express as px\n",
    "box,mask = create_mask_for_bounding_box(movie, ROI[6])\n",
    "px.imshow(box[0:100], animation_frame=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sthom215_problem_3_ex_1 import get_hw3_roi #get rois from exercise 3\n",
    "from sthom215_problem_4_ex_1 import create_mask_for_bounding_box, ROI\n",
    "import plotly.express as px\n",
    "box,mask = create_mask_for_bounding_box(movie, ROI[13])\n",
    "px.imshow(box[0:100], animation_frame=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sthom215_problem_3_ex_1 import get_hw3_roi #get rois from exercise 3\n",
    "from sthom215_problem_4_ex_1 import create_mask_for_bounding_box, ROI\n",
    "import plotly.express as px\n",
    "box,mask = create_mask_for_bounding_box(movie, ROI[1])\n",
    "px.imshow(box[0:100], animation_frame=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sthom215_problem_3_ex_1 import get_hw3_roi #get rois from exercise 3\n",
    "from sthom215_problem_4_ex_1 import create_mask_for_bounding_box, ROI\n",
    "import plotly.express as px\n",
    "box,mask = create_mask_for_bounding_box(movie, ROI[7])\n",
    "px.imshow(box[0:100], animation_frame=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Time-trace estimation\n",
    "\n",
    "With the ROIs, time-traces need to be extracted by identifying the relative brightness of that region\n",
    "of interest in each frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: \n",
    "\n",
    "* Do you think that the time-traces reflect the actual activity in the video? How might you test if they are accurate?\n",
    "\n",
    "\n",
    "I think that the time traces reflect what is actually  happening in the video to the extent that the ROI in the video is accurate. The less accurate the ROI, the worse the representation of the time traces is.\n",
    "\n",
    "\n",
    "Similar to checking how a single frame has an accurate ROI, we can employ experts to check the time traces or we can have many people check them and compare their output. These have the advantage that they could allow for fine tuning but both are expensive time wise with the former being just expensive in general. We can again use statistical methods as well, but errors in some of the frames could propagate to other frames as well, worsening the time traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Matrix factorization \n",
    "The above procedure is common in many labs, however can cause errors due to the dependency on\n",
    "the summary image and the multiple stages of processing. Matrix factorization has emerged as an\n",
    "alternative approach for identifying ROIs from the full spatio-temporal video. Here we will explore\n",
    "three different types of factorization and compare the results using a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Run PCA on the pixels-by-time matrix obtained by vectorizing each frame into a column\n",
    "vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load the video file using the provided TIFF_IO class.\n",
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "import plotly.express as px\n",
    "fname = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "tiffreader = TIFF_IO(home/fname)\n",
    "movie = tiffreader.read_tiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_1 import get_hw5_1_solution\n",
    "sol1 = get_hw5_1_solution(movie, num_components=2)\n",
    "px.imshow(sol1, animation_frame=0, binary_string=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_1 import  PCA\n",
    "\n",
    "pca1 = PCA(2)\n",
    "trans1 = pca1.fit_transform(movie)\n",
    "px.imshow(trans1, animation_frame=0, binary_string=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.skree_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the result change with the number of principal components you choose?\n",
    "\n",
    "With variance explained by the top 20 eigenvectors and the top 3, there is very litte if any change the pixels of reconstructed plot. Ineed, looking at the screeplot shows that the first 5 eigenvalues, and therefore the top 5 eigenvectors, dominate over the following eigenvalues. This means that there should be a change in the reconstructions betwen using the top 3 and 1 eigenvectors, but not really much after that. In the context of the movie, this means that the tiff can be condensned into approximate a 1-5 images that summarize the variance across the pixels for the entire movie. \n",
    "\n",
    "Running the last cold block, one can see that 8 and 3 look similar and as we plot higher number components, the detail represented in them becomes finer and finer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_1 import get_hw5_1_solution\n",
    "sol1 = get_hw5_1_solution(movie, num_components=10)\n",
    "px.imshow(sol1, animation_frame=0, binary_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Matrix factorization \n",
    "The above procedure is common in many labs, however can cause errors due to the dependency on\n",
    "the summary image and the multiple stages of processing. Matrix factorization has emerged as an\n",
    "alternative approach for identifying ROIs from the full spatio-temporal video. Here we will explore\n",
    "three different types of factorization and compare the results using a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "RRun NMF on the pixels-by-time matrix as in part A, for a specific number of components\n",
    "that you find reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load the video file using the provided TIFF_IO class.\n",
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "\n",
    "fname = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "tiffreader = TIFF_IO(home/fname)\n",
    "movie = tiffreader.read_tiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_2 import get_hw5_2_solution\n",
    "import plotly_express as px\n",
    "nmf_solution = get_hw5_2_solution(movie, 20)\n",
    "px.imshow(nmf_solution, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_2 import get_hw5_2_solution\n",
    "import plotly_express as px\n",
    "nmf_solution = get_hw5_2_solution(movie, 10)\n",
    "px.imshow(nmf_solution, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_2 import get_hw5_2_solution\n",
    "import plotly_express as px\n",
    "nmf_solution = get_hw5_2_solution(movie, 5)\n",
    "px.imshow(nmf_solution, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sthom215_problem_5_ex_2 import get_hw5_2_solution\n",
    "import plotly_express as px\n",
    "nmf_solution = get_hw5_2_solution(movie, 3)\n",
    "px.imshow(nmf_solution, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the differences that you note? Is there a similar dependency on the rank of the decomposition?\n",
    "\n",
    "I find that as the number of components of the factorization increases, the more each factor speacilizes in a single spatial set of pixels. The difference on the rank of the composition does not compress the pixels across time onto a single frame like with PCA. However, it does cause each image to finder a more granual neural activation pattern in each of the slices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Matrix factorization \n",
    "The above procedure is common in many labs, however can cause errors due to the dependency on\n",
    "the summary image and the multiple stages of processing. Matrix factorization has emerged as an\n",
    "alternative approach for identifying ROIs from the full spatio-temporal video. Here we will explore\n",
    "three different types of factorization and compare the results using a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Now try ICA for the same pixels-by-time matrix as in part A (for a specific number of\n",
    "components you find reasonable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load the video file using the provided TIFF_IO class.\n",
    "from sthom215_problem_1_ex_1 import TIFF_IO\n",
    "\n",
    "fname = 'TEST_MOVIE_00001-small-motion.tif'\n",
    "tiffreader = TIFF_IO(home/fname)\n",
    "movie = tiffreader.read_tiff()\n",
    "print(\"movie has shape:\", movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sthom215_problem_5_ex_3 import get_hw5_3_solution\n",
    "num_components = 4\n",
    "component_frames = get_hw5_3_solution(movie, num_components)\n",
    "px.imshow(component_frames, animation_frame=0, binary_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the differences that you note?\n",
    "\n",
    "Each of the frames have both different colors a well as different promient featuers. This refelcts ICA's focus on finding statistical indepdence between signals. Each frame represents a different component that the algorithm fit with the its gaussian independence assumption. With four components, one component focuses on vascularity in the frames while another looks liek it focuses on the nuclei in the frames across different cells. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
